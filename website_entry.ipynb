{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### all of the imports go into requirements!!!\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from datetime import timedelta\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "from ticket_control.params import path_to_data\n",
    "\n",
    "# Load your existing database into a DataFrame\n",
    "# Use flexible path so that it works on everyone's environment\n",
    "\n",
    "# Chris Notes: Functions are applied on data. Not good practice to load the data inside of functions.\n",
    "data = pd.read_csv(str(path_to_data) + \"/database_telegram.csv\", low_memory=False)\n",
    "\n",
    "\n",
    "##Chris Notes: Define the input of functions and declare their datatype.\n",
    "def data_preprocessing(data: pd.DataFrame):\n",
    "    # Provide a Doc String why we have this function and what it does in simple terms.\n",
    "    \"\"\"This function is the first step in our Datapreprocessing pipeline. It takes the Telegram Database with the columns....\"\"\"\n",
    "\n",
    "    # Notice the .copy() to copy the values\n",
    "    data = data.copy()\n",
    "\n",
    "    # replace sender type with str type\n",
    "    data[\"sender\"] = data[\"sender\"].astype(str)\n",
    "    \n",
    "    data[\"date\"] = data[\"date\"].str.strip(\"+00:00\").str[0:16]\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"], errors=\"coerce\")\n",
    "    # first round of cleaning na/empty strings/...\n",
    "    data = data[data[\"text\"].notna()]\n",
    "    data[\"text\"] = data[\"text\"].str.strip()\n",
    "    data[\"text\"].replace(\"\", np.nan, inplace=True)\n",
    "    data.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "    # sorting values by sender & date\n",
    "    df = data.sort_values(by=[\"sender\", \"date\"])\n",
    "\n",
    "    # creating a time difference\n",
    "    df[\"time_diff\"] = df.groupby(\"sender\")[\"date\"].diff()\n",
    "\n",
    "    #\n",
    "    data_clean = (\n",
    "        df.groupby([\"sender\", (df[\"time_diff\"] > timedelta(minutes=10)).cumsum()])\n",
    "        .agg({\"text\": \" \".join, \"date\": \"first\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    data_clean[\"sender\"] = data_clean[\"sender\"].astype(str)\n",
    "    # Chris Notes: Always import at the start of the Module.\n",
    "    import re\n",
    "\n",
    "    def remove_emojis(data):\n",
    "        emoj = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            \"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            \"\\U00002702-\\U000027B0\"\n",
    "            \"\\U00002702-\\U000027B0\"\n",
    "            \"\\U000024C2-\\U0001F251\"\n",
    "            \"\\U0001f926-\\U0001f937\"\n",
    "            \"\\U00010000-\\U0010ffff\"\n",
    "            \"\\u2640-\\u2642\"\n",
    "            \"\\u2600-\\u2B55\"\n",
    "            \"\\u200d\"\n",
    "            \"\\u23cf\"\n",
    "            \"\\u23e9\"\n",
    "            \"\\u231a\"\n",
    "            \"\\ufe0f\"  # dingbats\n",
    "            \"\\u3030\"\n",
    "            \"]+\",\n",
    "            re.UNICODE,\n",
    "        )\n",
    "        return re.sub(emoj, \"\", data)\n",
    "\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].apply(lambda x: remove_emojis(str(x)))\n",
    "\n",
    "    # second round of cleaning na/empty strings/...\n",
    "    data_clean = data_clean[data_clean[\"text\"].notna()]\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.strip()\n",
    "    data_clean[\"text\"].replace(\"\", np.nan, inplace=True)\n",
    "    data_clean.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "    data_clean = data_clean.drop_duplicates()\n",
    "\n",
    "    # lowercasing all strings\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].apply(lambda x: x.lower())\n",
    "\n",
    "    # generating list of default stop words\n",
    "    stop_words = set(stopwords.words(\"german\"))\n",
    "\n",
    "    # add multiple words using 'update'\n",
    "    new_words_to_add = [\n",
    "        \"männlich\",\n",
    "        \"weiblich\",\n",
    "        \"gelesen\",\n",
    "        \"weste\",\n",
    "        \"westen\",\n",
    "        \"shirt\",\n",
    "        \"pulli\",\n",
    "        \"jacke\",\n",
    "        \"jacken\",\n",
    "        \"ticket\",\n",
    "        \"tickets\",\n",
    "        \"eingestiegen\",\n",
    "        \"ausgestiegen\",\n",
    "        \"steigen\",\n",
    "        \"schwarze\",\n",
    "        \"schwarz\",\n",
    "        \"männer\",\n",
    "        \"haare\",\n",
    "        \"the\",\n",
    "        \"stehen\",\n",
    "        \"gelesene\",\n",
    "        \"blaue\",\n",
    "        \"with\",\n",
    "        \"wertend\",\n",
    "        \"fahrgaesten\",\n",
    "        \"fahrgaeste\",\n",
    "        \"fahrgästen\",\n",
    "        \"fahrgästen\",\n",
    "        \"westlichen\",\n",
    "        \"warnwesten\",\n",
    "        \"gelbwesten\",\n",
    "        \"abwertend\",\n",
    "        \"blauwesten\",\n",
    "        \"fahrgaesten\",\n",
    "        \"wertende\",\n",
    "        \"besten\",\n",
    "        \"nichtwertende\",\n",
    "        \"wuetend\",\n",
    "        \"wütend\",\n",
    "        \"wuetend\",\n",
    "        \"wuetenden\",\n",
    "        \"genau\",\n",
    "        \"sicher\",\n",
    "        \"ungenau\",\n",
    "        \"sicherheitswesten\",\n",
    "        \"westentraeger\",\n",
    "    ]\n",
    "    stop_words.update(new_words_to_add)\n",
    "\n",
    "    # Remove unwanted stopwords\n",
    "    my_wanted_words = [\"nach\", \"bei\", \"von\", \"vom\" \"zum\", \"über\", \"bis\"]\n",
    "    final_stopwords = stop_words - set(my_wanted_words)\n",
    "\n",
    "    # Chris Notes: Not best practice to define functions inside of funcitons. Better to keep the definition separate and call the function within other functions.\n",
    "    def stopword(text):\n",
    "        word_tokens = word_tokenize(text)\n",
    "        text = [\n",
    "            w for w in word_tokens if not w in final_stopwords\n",
    "        ]  ## if w isn't in final_stopwords, return w\n",
    "        return \" \".join(text)  ##transforming list into string again\n",
    "\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].apply(stopword)\n",
    "\n",
    "    # removing punctuation\n",
    "    for element in string.punctuation:\n",
    "        data_clean[\"text\"] = data_clean[\"text\"].str.replace(element, \"\")\n",
    "\n",
    "    # third round of cleaning na/empty strings/...\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].replace(\"\", np.nan)\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.strip()\n",
    "    data_clean.dropna(subset=\"text\", inplace=True)\n",
    "    data_clean = data_clean.drop_duplicates(subset=\"text\")\n",
    "    data_clean = data_clean[data_clean[\"text\"] != \"\"]\n",
    "    data_clean.dropna(subset=\"text\", inplace=True)\n",
    "\n",
    "    # replacing unwanted characters and words\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"ß\", \"ss\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"ä\", \"ae\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"ö\", \"oe\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"ü\", \"ue\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"strasse\", \"str\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"alexanderplatz\", \"alex\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"zoologischer garten\", \"zoo\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"kottbusser\", \"kotti\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"goerlitzer\", \"goerli\")\n",
    "\n",
    "    # final sorting\n",
    "    data_clean = data_clean.sort_values(by=[\"date\", \"sender\"])\n",
    "\n",
    "    # converting into \"handover\" file\n",
    "    ##Chris Notes: Assign you objects names that indicate their type and state in the process.\n",
    "    df_for_fuzzy_matching = data_clean.drop(\"time_diff\", axis=1)\n",
    "\n",
    "    return df_for_fuzzy_matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from thefuzz import process\n",
    "from thefuzz import fuzz\n",
    "import re\n",
    "from ticket_control.data_preprocessing import data_preprocessing\n",
    "from ticket_control.params import path_to_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Chris Notes: Name your variables after the type of the variable and their purpose in the pipeline.\n",
    "df_station_mapping = pd.read_csv(\n",
    "    str(path_to_data) + \"/s_u_stations_fixed_with_keys_20230830.csv\"\n",
    ")  # Replace with the path to your database file\n",
    "\n",
    "\n",
    "# Christ Notes: Better to define the function outside of the function. We want to create individual units of code that are testable and serve a single purpose.\n",
    "def create_station_to_line_df(df_station_mapping: pd.DataFrame):\n",
    "    # df = pd.read_csv('s_u_stations_fixed_with_keys_20230830.csv')  # Replace with the path to your database file\n",
    "    df = df_station_mapping.copy()\n",
    "\n",
    "    # create a dictionary where U/S bahn line names are the keys and the respective stations are the values incl. lat & lon\n",
    "    output = {\"station_key\": [], \"line\": []}\n",
    "    for idx, row in df.iterrows():\n",
    "        line_split = row[\"lines\"].split(\", \")\n",
    "        for i in line_split:\n",
    "            output[\"station_key\"].append(row[\"keys\"])\n",
    "            output[\"line\"].append(i)\n",
    "    station_to_line = pd.DataFrame(output)\n",
    "    station_to_line = station_to_line.drop_duplicates()\n",
    "    return station_to_line\n",
    "\n",
    "\n",
    "def fuzz_flow(df_for_fuzzy_matching: pd.DataFrame, station_to_line: pd.DataFrame):\n",
    "    # Chris Notes: Always write a short Docstring for your function, describe what it does and what is input and outputs are.\n",
    "    \"\"\"Docstring for this function, This function does x,y,z...\"\"\"\n",
    "    # Chris Notes: We want to separate the individual cleaning steps into different functions and states. This makes it easier to track down errors.\n",
    "    # Better to take the input from yannik and use it as a direct input into your function than to call his function agian.\n",
    "    data3 = df_for_fuzzy_matching\n",
    "    data3 = data3.copy()\n",
    "\n",
    "    # Load STATIONS DATAFRAME\n",
    "    station_to_line = station_to_line.copy()\n",
    "\n",
    "    df = pd.read_csv(str(path_to_data) + \"/s_u_stations_fixed_with_keys_20230830.csv\")\n",
    "\n",
    "    lines_un = list(station_to_line[\"line\"].unique())\n",
    "    stations_full = list(df[\"keys\"].values)\n",
    "\n",
    "    # Chris Notes: Better to be defined outside of function.\n",
    "    def identify_station_precise(\n",
    "        some_string, confidence_first=80, confidence_second=90\n",
    "    ):\n",
    "        res1 = None\n",
    "        res2 = None\n",
    "        if some_string[1][1] > confidence_second:\n",
    "            res1 = some_string[1][0]\n",
    "            return some_string[0][0], some_string[1][0]\n",
    "        elif (\n",
    "            some_string[0][1] > confidence_first\n",
    "        ):  # try 79 or 89 and other, better less lines but better quality\n",
    "            return some_string[0][0]\n",
    "        return None\n",
    "\n",
    "    # Chris Notes: Better to be defined outside of function.\n",
    "    def station_finder(some_string):\n",
    "        for line in lines_un:\n",
    "            matches = re.search(r\"{line}[^0-9]\".format(line=line.lower()), some_string)\n",
    "            if matches is not None:\n",
    "                stations = list(\n",
    "                    station_to_line[station_to_line[\"line\"] == line][\"station_key\"]\n",
    "                )\n",
    "                out = process.extract(\n",
    "                    some_string, stations, limit=2, scorer=fuzz.partial_ratio\n",
    "                )\n",
    "                return identify_station_precise(out, 70, 70)\n",
    "        out = process.extract(\n",
    "            some_string, stations_full, limit=2, scorer=fuzz.partial_ratio\n",
    "        )\n",
    "        return identify_station_precise(out)\n",
    "\n",
    "    df_chat = data3[[\"date\"]]\n",
    "\n",
    "    df_chat[\"station_key\"] = data3[\"text\"].map(station_finder)\n",
    "    df_chat[\"text\"] = data3[\"text\"]\n",
    "    df_chat.dropna(subset=\"station_key\", inplace=True)\n",
    "    full_df = df_chat.merge(df, left_on=\"station_key\", right_on=\"keys\")\n",
    "    full_df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "    full_df.drop(columns=\"keys\", inplace=True)\n",
    "    full_df = full_df.sort_index(ascending=True)\n",
    "    return full_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>group</th>\n",
       "      <th>sender</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118379</th>\n",
       "      <td>0</td>\n",
       "      <td>website</td>\n",
       "      <td>2023-09-04 07:27:37</td>\n",
       "      <td>Heerstraße</td>\n",
       "      <td>2023-09-04 07:27:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0    group               sender        text  \\\n",
       "118379           0  website  2023-09-04 07:27:37  Heerstraße   \n",
       "\n",
       "                       date  \n",
       "118379  2023-09-04 07:27:37  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(str(path_to_data) + \"/database_telegram.csv\", low_memory=False)\n",
    "data = data[data['group'] ==  'website'].iloc[-1:,:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0    group               sender        text  \\\n",
      "118379           0  website  2023-09-04 07:27:37  Heerstraße   \n",
      "\n",
      "                      date  \n",
      "118379 2023-09-04 07:27:00  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-04 07:27:37</td>\n",
       "      <td>heerstr</td>\n",
       "      <td>2023-09-04 07:27:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sender     text                date\n",
       "0  2023-09-04 07:27:37  heerstr 2023-09-04 07:27:00"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_preprocessing(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_314355/1273296437.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_chat[\"station_key\"] = data3[\"text\"].map(station_finder)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station_key</th>\n",
       "      <th>text</th>\n",
       "      <th>station name</th>\n",
       "      <th>lines</th>\n",
       "      <th>area</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-04 07:27:00</td>\n",
       "      <td>heerstr</td>\n",
       "      <td>heerstr</td>\n",
       "      <td>Heerstraße</td>\n",
       "      <td>S3, S9</td>\n",
       "      <td>Westend</td>\n",
       "      <td>52.508611</td>\n",
       "      <td>13.258611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date station_key     text station name   lines     area  \\\n",
       "0 2023-09-04 07:27:00     heerstr  heerstr   Heerstraße  S3, S9  Westend   \n",
       "\n",
       "    latitude  longitude  \n",
       "0  52.508611  13.258611  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_to_line = create_station_to_line_df(df_station_mapping)\n",
    "df = fuzz_flow(df_for_fuzzy_matching=df, station_to_line=station_to_line)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_station_colors(from_date: str, to_date: str) -> pd.DataFrame:\n",
    "    \"\"\"This functions returns the Dataframe for the Map of the Streamlit App.\n",
    "    It takes the input preprocessed Database that is filtered on user input date\n",
    "    range and returns the reports form the relevant time period.\n",
    "    All reported stations will appear red on the Map.\n",
    "    The from and to dateformat ,e.g., from_date='2023-08-30 11:55:00'\n",
    "    to_date='2023-08-30 12:01:00'.\"\"\"\n",
    "    # Read data from CSV files\n",
    "    reports = pd.read_csv(\n",
    "        str(path_to_main) + \"/data/preprocessed_database_telegram.csv\"\n",
    "    )\n",
    "    stations = pd.read_csv(str(path_to_main) + \"/data/datanew_map2.csv\")\n",
    "    # Filter reports based on date\n",
    "    reports = reports.copy()\n",
    "    stations = stations.copy()\n",
    "    reports_filtered = reports[\n",
    "        (reports[\"date\"] >= from_date) & (reports[\"date\"] <= to_date)\n",
    "    ]\n",
    "    print(reports_filtered.iloc[-5:])\n",
    "    # Loop through unique station names in the filtered reports\n",
    "    for report_station in reports_filtered[\"station name\"].unique():\n",
    "        # Update the 'color' column for matching stations to '#FF0000'\n",
    "        stations.loc[stations[\"station name\"] == report_station, \"color\"] = \"#FF0000\"\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0                 date         station_key  \\\n",
      "60389          34  2023-09-01 22:58:00     frankfurter tor   \n",
      "60390          35  2023-09-02 07:25:00          wollankstr   \n",
      "60391          36  2023-09-02 22:15:00          hermannstr   \n",
      "60392          37  2023-09-03 06:22:00  heidelberger platz   \n",
      "60393          38  2023-09-04 07:27:00             heerstr   \n",
      "\n",
      "                                                    text        station name  \\\n",
      "60389  m10 frankfurter tor m10 frankfurter tor m10 fr...     Frankfurter Tor   \n",
      "60390  s wollankstr zweimal gleis s wollankstr zweima...       Wollankstraße   \n",
      "60391  ring gerade s hermannstr 1x m ring gerade s he...       Hermannstraße   \n",
      "60392  ringbahn s42 heidelberger platz 2x w ringbahn ...  Heidelberger Platz   \n",
      "60393                                            heerstr          Heerstraße   \n",
      "\n",
      "                             lines            area   latitude  longitude  \n",
      "60389                           U5  Friedrichshain  52.515833  13.454167  \n",
      "60390                 S1, S25, S26          Pankow  52.565278  13.392222  \n",
      "60391  U8, S41, S42, S45, S46, S47        Neukölln  52.467500  13.431250  \n",
      "60392            U3, S41, S42, S46     Wilmersdorf  52.480000  13.311944  \n",
      "60393                       S3, S9         Westend  52.508611  13.258611  \n"
     ]
    }
   ],
   "source": [
    "path_to_main = '/home/chris/OneDrive_christoph.bieritz/code/ticket-control-bvg/'\n",
    "\n",
    "df_filtered_map = update_station_colors(\n",
    "    from_date=\"2023-09-01 12:28:00\",  # Insert Sliders Dates here!\n",
    "    to_date=\"2023-09-10 10:28:00\",  # Insert Sliders Dates here!\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bvg-controller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
