{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### all of the imports go into requirements!!!\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from datetime import timedelta\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "from ticket_control.params import path_to_data\n",
    "\n",
    "# Load your existing database into a DataFrame\n",
    "# Use flexible path so that it works on everyone's environment\n",
    "\n",
    "# Chris Notes: Functions are applied on data. Not good practice to load the data inside of functions.\n",
    "data = pd.read_csv(str(path_to_data) + \"/database_telegram.csv\", low_memory=False)\n",
    "\n",
    "\n",
    "##Chris Notes: Define the input of functions and declare their datatype.\n",
    "def data_preprocessing(data: pd.DataFrame):\n",
    "    # Provide a Doc String why we have this function and what it does in simple terms.\n",
    "    \"\"\"This function is the first step in our Datapreprocessing pipeline. It takes the Telegram Database with the columns....\"\"\"\n",
    "\n",
    "    # Notice the .copy() to copy the values\n",
    "    data = data.copy()\n",
    "\n",
    "    # replace sender type with str type\n",
    "    data[\"sender\"] = data[\"sender\"].astype(str)\n",
    "    \n",
    "    data[\"date\"] = data[\"date\"].str.strip(\"+00:00\").str[0:16]\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"], errors=\"coerce\")\n",
    "    # first round of cleaning na/empty strings/...\n",
    "    data = data[data[\"text\"].notna()]\n",
    "    data[\"text\"] = data[\"text\"].str.strip()\n",
    "    data[\"text\"].replace(\"\", np.nan, inplace=True)\n",
    "    data.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "    # sorting values by sender & date\n",
    "    df = data.sort_values(by=[\"sender\", \"date\"])\n",
    "\n",
    "    # creating a time difference\n",
    "    df[\"time_diff\"] = df.groupby(\"sender\")[\"date\"].diff()\n",
    "\n",
    "    #\n",
    "    data_clean = (\n",
    "        df.groupby([\"sender\", (df[\"time_diff\"] > timedelta(minutes=10)).cumsum()])\n",
    "        .agg({\"text\": \" \".join, \"date\": \"first\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    data_clean[\"sender\"] = data_clean[\"sender\"].astype(str)\n",
    "    # Chris Notes: Always import at the start of the Module.\n",
    "    import re\n",
    "\n",
    "    def remove_emojis(data):\n",
    "        emoj = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            \"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            \"\\U00002702-\\U000027B0\"\n",
    "            \"\\U00002702-\\U000027B0\"\n",
    "            \"\\U000024C2-\\U0001F251\"\n",
    "            \"\\U0001f926-\\U0001f937\"\n",
    "            \"\\U00010000-\\U0010ffff\"\n",
    "            \"\\u2640-\\u2642\"\n",
    "            \"\\u2600-\\u2B55\"\n",
    "            \"\\u200d\"\n",
    "            \"\\u23cf\"\n",
    "            \"\\u23e9\"\n",
    "            \"\\u231a\"\n",
    "            \"\\ufe0f\"  # dingbats\n",
    "            \"\\u3030\"\n",
    "            \"]+\",\n",
    "            re.UNICODE,\n",
    "        )\n",
    "        return re.sub(emoj, \"\", data)\n",
    "\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].apply(lambda x: remove_emojis(str(x)))\n",
    "\n",
    "    # second round of cleaning na/empty strings/...\n",
    "    data_clean = data_clean[data_clean[\"text\"].notna()]\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.strip()\n",
    "    data_clean[\"text\"].replace(\"\", np.nan, inplace=True)\n",
    "    data_clean.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "    data_clean = data_clean.drop_duplicates()\n",
    "\n",
    "    # lowercasing all strings\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].apply(lambda x: x.lower())\n",
    "\n",
    "    # generating list of default stop words\n",
    "    stop_words = set(stopwords.words(\"german\"))\n",
    "\n",
    "    # add multiple words using 'update'\n",
    "    new_words_to_add = [\n",
    "        \"männlich\",\n",
    "        \"weiblich\",\n",
    "        \"gelesen\",\n",
    "        \"weste\",\n",
    "        \"westen\",\n",
    "        \"shirt\",\n",
    "        \"pulli\",\n",
    "        \"jacke\",\n",
    "        \"jacken\",\n",
    "        \"ticket\",\n",
    "        \"tickets\",\n",
    "        \"eingestiegen\",\n",
    "        \"ausgestiegen\",\n",
    "        \"steigen\",\n",
    "        \"schwarze\",\n",
    "        \"schwarz\",\n",
    "        \"männer\",\n",
    "        \"haare\",\n",
    "        \"the\",\n",
    "        \"stehen\",\n",
    "        \"gelesene\",\n",
    "        \"blaue\",\n",
    "        \"with\",\n",
    "        \"wertend\",\n",
    "        \"fahrgaesten\",\n",
    "        \"fahrgaeste\",\n",
    "        \"fahrgästen\",\n",
    "        \"fahrgästen\",\n",
    "        \"westlichen\",\n",
    "        \"warnwesten\",\n",
    "        \"gelbwesten\",\n",
    "        \"abwertend\",\n",
    "        \"blauwesten\",\n",
    "        \"fahrgaesten\",\n",
    "        \"wertende\",\n",
    "        \"besten\",\n",
    "        \"nichtwertende\",\n",
    "        \"wuetend\",\n",
    "        \"wütend\",\n",
    "        \"wuetend\",\n",
    "        \"wuetenden\",\n",
    "        \"genau\",\n",
    "        \"sicher\",\n",
    "        \"ungenau\",\n",
    "        \"sicherheitswesten\",\n",
    "        \"westentraeger\",\n",
    "    ]\n",
    "    stop_words.update(new_words_to_add)\n",
    "\n",
    "    # Remove unwanted stopwords\n",
    "    my_wanted_words = [\"nach\", \"bei\", \"von\", \"vom\" \"zum\", \"über\", \"bis\"]\n",
    "    final_stopwords = stop_words - set(my_wanted_words)\n",
    "\n",
    "    # Chris Notes: Not best practice to define functions inside of funcitons. Better to keep the definition separate and call the function within other functions.\n",
    "    def stopword(text):\n",
    "        word_tokens = word_tokenize(text)\n",
    "        text = [\n",
    "            w for w in word_tokens if not w in final_stopwords\n",
    "        ]  ## if w isn't in final_stopwords, return w\n",
    "        return \" \".join(text)  ##transforming list into string again\n",
    "\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].apply(stopword)\n",
    "\n",
    "    # removing punctuation\n",
    "    for element in string.punctuation:\n",
    "        data_clean[\"text\"] = data_clean[\"text\"].str.replace(element, \"\")\n",
    "\n",
    "    # third round of cleaning na/empty strings/...\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].replace(\"\", np.nan)\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.strip()\n",
    "    data_clean.dropna(subset=\"text\", inplace=True)\n",
    "    data_clean = data_clean.drop_duplicates(subset=\"text\")\n",
    "    data_clean = data_clean[data_clean[\"text\"] != \"\"]\n",
    "    data_clean.dropna(subset=\"text\", inplace=True)\n",
    "\n",
    "    # replacing unwanted characters and words\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"ß\", \"ss\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"ä\", \"ae\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"ö\", \"oe\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"ü\", \"ue\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"strasse\", \"str\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"alexanderplatz\", \"alex\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"zoologischer garten\", \"zoo\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"kottbusser\", \"kotti\")\n",
    "    data_clean[\"text\"] = data_clean[\"text\"].str.replace(\"goerlitzer\", \"goerli\")\n",
    "\n",
    "    # final sorting\n",
    "    data_clean = data_clean.sort_values(by=[\"date\", \"sender\"])\n",
    "\n",
    "    # converting into \"handover\" file\n",
    "    ##Chris Notes: Assign you objects names that indicate their type and state in the process.\n",
    "    df_for_fuzzy_matching = data_clean.drop(\"time_diff\", axis=1)\n",
    "\n",
    "    return df_for_fuzzy_matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from thefuzz import process\n",
    "from thefuzz import fuzz\n",
    "import re\n",
    "from ticket_control.data_preprocessing import data_preprocessing\n",
    "from ticket_control.params import path_to_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Chris Notes: Name your variables after the type of the variable and their purpose in the pipeline.\n",
    "df_station_mapping = pd.read_csv(\n",
    "    str(path_to_data) + \"/s_u_stations_fixed_with_keys_20230830.csv\"\n",
    ")  # Replace with the path to your database file\n",
    "\n",
    "\n",
    "# Christ Notes: Better to define the function outside of the function. We want to create individual units of code that are testable and serve a single purpose.\n",
    "def create_station_to_line_df(df_station_mapping: pd.DataFrame):\n",
    "    # df = pd.read_csv('s_u_stations_fixed_with_keys_20230830.csv')  # Replace with the path to your database file\n",
    "    df = df_station_mapping.copy()\n",
    "\n",
    "    # create a dictionary where U/S bahn line names are the keys and the respective stations are the values incl. lat & lon\n",
    "    output = {\"station_key\": [], \"line\": []}\n",
    "    for idx, row in df.iterrows():\n",
    "        line_split = row[\"lines\"].split(\", \")\n",
    "        for i in line_split:\n",
    "            output[\"station_key\"].append(row[\"keys\"])\n",
    "            output[\"line\"].append(i)\n",
    "    station_to_line = pd.DataFrame(output)\n",
    "    station_to_line = station_to_line.drop_duplicates()\n",
    "    return station_to_line\n",
    "\n",
    "\n",
    "def fuzz_flow(df_for_fuzzy_matching: pd.DataFrame, station_to_line: pd.DataFrame):\n",
    "    # Chris Notes: Always write a short Docstring for your function, describe what it does and what is input and outputs are.\n",
    "    \"\"\"Docstring for this function, This function does x,y,z...\"\"\"\n",
    "    # Chris Notes: We want to separate the individual cleaning steps into different functions and states. This makes it easier to track down errors.\n",
    "    # Better to take the input from yannik and use it as a direct input into your function than to call his function agian.\n",
    "    data3 = df_for_fuzzy_matching\n",
    "    data3 = data3.copy()\n",
    "\n",
    "    # Load STATIONS DATAFRAME\n",
    "    station_to_line = station_to_line.copy()\n",
    "\n",
    "    df = pd.read_csv(str(path_to_data) + \"/s_u_stations_fixed_with_keys_20230830.csv\")\n",
    "\n",
    "    lines_un = list(station_to_line[\"line\"].unique())\n",
    "    stations_full = list(df[\"keys\"].values)\n",
    "\n",
    "    # Chris Notes: Better to be defined outside of function.\n",
    "    def identify_station_precise(\n",
    "        some_string, confidence_first=80, confidence_second=90\n",
    "    ):\n",
    "        res1 = None\n",
    "        res2 = None\n",
    "        if some_string[1][1] > confidence_second:\n",
    "            res1 = some_string[1][0]\n",
    "            return some_string[0][0], some_string[1][0]\n",
    "        elif (\n",
    "            some_string[0][1] > confidence_first\n",
    "        ):  # try 79 or 89 and other, better less lines but better quality\n",
    "            return some_string[0][0]\n",
    "        return None\n",
    "\n",
    "    # Chris Notes: Better to be defined outside of function.\n",
    "    def station_finder(some_string):\n",
    "        for line in lines_un:\n",
    "            matches = re.search(r\"{line}[^0-9]\".format(line=line.lower()), some_string)\n",
    "            if matches is not None:\n",
    "                stations = list(\n",
    "                    station_to_line[station_to_line[\"line\"] == line][\"station_key\"]\n",
    "                )\n",
    "                out = process.extract(\n",
    "                    some_string, stations, limit=2, scorer=fuzz.partial_ratio\n",
    "                )\n",
    "                return identify_station_precise(out, 70, 70)\n",
    "        out = process.extract(\n",
    "            some_string, stations_full, limit=2, scorer=fuzz.partial_ratio\n",
    "        )\n",
    "        return identify_station_precise(out)\n",
    "\n",
    "    df_chat = data3[[\"date\"]]\n",
    "\n",
    "    df_chat[\"station_key\"] = data3[\"text\"].map(station_finder)\n",
    "    df_chat[\"text\"] = data3[\"text\"]\n",
    "    df_chat.dropna(subset=\"station_key\", inplace=True)\n",
    "    full_df = df_chat.merge(df, left_on=\"station_key\", right_on=\"keys\")\n",
    "    full_df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "    full_df.drop(columns=\"keys\", inplace=True)\n",
    "    full_df = full_df.sort_index(ascending=True)\n",
    "    return full_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>group</th>\n",
       "      <th>sender</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118379</th>\n",
       "      <td>0</td>\n",
       "      <td>website</td>\n",
       "      <td>2023-09-04 07:27:37</td>\n",
       "      <td>Heerstraße</td>\n",
       "      <td>2023-09-04 07:27:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0    group               sender        text  \\\n",
       "118379           0  website  2023-09-04 07:27:37  Heerstraße   \n",
       "\n",
       "                       date  \n",
       "118379  2023-09-04 07:27:37  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(str(path_to_data) + \"/database_telegram.csv\", low_memory=False)\n",
    "data = data[data['group'] ==  'website'].iloc[-1:,:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0    group               sender        text  \\\n",
      "118379           0  website  2023-09-04 07:27:37  Heerstraße   \n",
      "\n",
      "                      date  \n",
      "118379 2023-09-04 07:27:00  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-04 07:27:37</td>\n",
       "      <td>heerstr</td>\n",
       "      <td>2023-09-04 07:27:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sender     text                date\n",
       "0  2023-09-04 07:27:37  heerstr 2023-09-04 07:27:00"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_preprocessing(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_314355/1273296437.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_chat[\"station_key\"] = data3[\"text\"].map(station_finder)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station_key</th>\n",
       "      <th>text</th>\n",
       "      <th>station name</th>\n",
       "      <th>lines</th>\n",
       "      <th>area</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-04 07:27:00</td>\n",
       "      <td>heerstr</td>\n",
       "      <td>heerstr</td>\n",
       "      <td>Heerstraße</td>\n",
       "      <td>S3, S9</td>\n",
       "      <td>Westend</td>\n",
       "      <td>52.508611</td>\n",
       "      <td>13.258611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date station_key     text station name   lines     area  \\\n",
       "0 2023-09-04 07:27:00     heerstr  heerstr   Heerstraße  S3, S9  Westend   \n",
       "\n",
       "    latitude  longitude  \n",
       "0  52.508611  13.258611  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_to_line = create_station_to_line_df(df_station_mapping)\n",
    "df = fuzz_flow(df_for_fuzzy_matching=df, station_to_line=station_to_line)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_station_colors(from_date: str, to_date: str) -> pd.DataFrame:\n",
    "    \"\"\"This functions returns the Dataframe for the Map of the Streamlit App.\n",
    "    It takes the input preprocessed Database that is filtered on user input date\n",
    "    range and returns the reports form the relevant time period.\n",
    "    All reported stations will appear red on the Map.\n",
    "    The from and to dateformat ,e.g., from_date='2023-08-30 11:55:00'\n",
    "    to_date='2023-08-30 12:01:00'.\"\"\"\n",
    "    # Read data from CSV files\n",
    "    reports = pd.read_csv(\n",
    "        str(path_to_main) + \"/data/preprocessed_database_telegram.csv\"\n",
    "    )\n",
    "    stations = pd.read_csv(str(path_to_main) + \"/data/datanew_map2.csv\")\n",
    "    # Filter reports based on date\n",
    "    reports = reports.copy()\n",
    "    stations = stations.copy()\n",
    "    reports_filtered = reports[\n",
    "        (reports[\"date\"] >= from_date) & (reports[\"date\"] <= to_date)\n",
    "    ]\n",
    "    print(reports_filtered.iloc[-5:])\n",
    "    # Loop through unique station names in the filtered reports\n",
    "    for report_station in reports_filtered[\"station name\"].unique():\n",
    "        # Update the 'color' column for matching stations to '#FF0000'\n",
    "        stations.loc[stations[\"station name\"] == report_station, \"color\"] = \"#FF0000\"\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mticket_control\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbig_query_download_processed\u001b[39;00m \u001b[39mimport\u001b[39;00m download_big_query_processed\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m----> 4\u001b[0m path_to_main \u001b[39m=\u001b[39m Path(\u001b[39m__file__\u001b[39;49m)\u001b[39m.\u001b[39mparent\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_station_colors\u001b[39m(from_date: \u001b[39mstr\u001b[39m, to_date: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m      7\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This functions returns the Dataframe for the Map of the Streamlit App.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m    It takes the input preprocessed Database that is filtered on user input date\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m    range and returns the reports form the relevant time period.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m    All reported stations will appear red on the Map.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    The from and to dateformat ,e.g., from_date='2023-08-30 11:55:00'\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m    to_date='2023-08-30 12:01:00'.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from ticket_control.big_query_download_processed import download_big_query_processed\n",
    "from pathlib import Path\n",
    "\n",
    "path_to_main = Path(__file__).parent\n",
    "\n",
    "def update_station_colors(from_date: str, to_date: str) -> pd.DataFrame:\n",
    "    \"\"\"This functions returns the Dataframe for the Map of the Streamlit App.\n",
    "    It takes the input preprocessed Database that is filtered on user input date\n",
    "    range and returns the reports form the relevant time period.\n",
    "    All reported stations will appear red on the Map.\n",
    "    The from and to dateformat ,e.g., from_date='2023-08-30 11:55:00'\n",
    "    to_date='2023-08-30 12:01:00'.\"\"\"\n",
    "    # Read data from CSV files\n",
    "    reports = download_big_query_processed()\n",
    "    stations = pd.read_csv(str(path_to_main) + \"/data/datanew_map2.csv\")\n",
    "    # Filter reports based on date\n",
    "    reports = reports.copy()\n",
    "    stations = stations.copy()\n",
    "    reports_filtered = reports[\n",
    "        (reports[\"date\"] >= from_date) & (reports[\"date\"] <= to_date)\n",
    "    ]\n",
    "    # Loop through unique station names in the filtered reports\n",
    "    for report_station in reports_filtered[\"station name\"].unique():\n",
    "        # Update the 'color' column for matching stations to '#FF0000'\n",
    "        stations.loc[stations[\"station name\"] == report_station, \"color\"] = \"#FF0000\"\n",
    "    return print(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "map = pd.read_csv('data/datanew_map2.csv')\n",
    "key = pd.read_csv('data/s_u_stations_fixed_with_keys_20230830.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>station name</th>\n",
       "      <th>lines</th>\n",
       "      <th>area</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Adenauerplatz</td>\n",
       "      <td>U7</td>\n",
       "      <td>Charlottenburg</td>\n",
       "      <td>52.499722</td>\n",
       "      <td>13.307222</td>\n",
       "      <td>adenauerplatz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adlershof</td>\n",
       "      <td>S45, S46, S8, S85, S9</td>\n",
       "      <td>Adlershof</td>\n",
       "      <td>52.434722</td>\n",
       "      <td>13.541389</td>\n",
       "      <td>adlershof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Afrikanische Straße</td>\n",
       "      <td>U6</td>\n",
       "      <td>Wedding</td>\n",
       "      <td>52.560556</td>\n",
       "      <td>13.334167</td>\n",
       "      <td>afrikanische str</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ahrensfelde</td>\n",
       "      <td>S7</td>\n",
       "      <td>Marzahn</td>\n",
       "      <td>52.571667</td>\n",
       "      <td>13.565000</td>\n",
       "      <td>ahrensfelde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Alexanderplatz</td>\n",
       "      <td>U2, U5, U8, S3, S5, S7, S9</td>\n",
       "      <td>Mitte</td>\n",
       "      <td>52.521389</td>\n",
       "      <td>13.412639</td>\n",
       "      <td>alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>313</td>\n",
       "      <td>Zepernick</td>\n",
       "      <td>S2</td>\n",
       "      <td>Panketal</td>\n",
       "      <td>52.659722</td>\n",
       "      <td>13.533889</td>\n",
       "      <td>zepernick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>314</td>\n",
       "      <td>Zeuthen</td>\n",
       "      <td>S46</td>\n",
       "      <td>Zeuthen</td>\n",
       "      <td>52.348611</td>\n",
       "      <td>13.627500</td>\n",
       "      <td>zeuthen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>315</td>\n",
       "      <td>Zitadelle</td>\n",
       "      <td>U7</td>\n",
       "      <td>Haselhorst</td>\n",
       "      <td>52.537778</td>\n",
       "      <td>13.217778</td>\n",
       "      <td>zitadelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>316</td>\n",
       "      <td>Zoologischer Garten</td>\n",
       "      <td>U2, U9, S3, S5, S7, S9</td>\n",
       "      <td>Charlottenburg</td>\n",
       "      <td>52.507222</td>\n",
       "      <td>13.332500</td>\n",
       "      <td>zoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>317</td>\n",
       "      <td>Zwickauer Damm</td>\n",
       "      <td>U7</td>\n",
       "      <td>Gropiusstadt</td>\n",
       "      <td>52.423333</td>\n",
       "      <td>13.483889</td>\n",
       "      <td>zwickauer damm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0         station name                       lines  \\\n",
       "0             0        Adenauerplatz                          U7   \n",
       "1             1            Adlershof       S45, S46, S8, S85, S9   \n",
       "2             2  Afrikanische Straße                          U6   \n",
       "3             3          Ahrensfelde                          S7   \n",
       "4             4       Alexanderplatz  U2, U5, U8, S3, S5, S7, S9   \n",
       "..          ...                  ...                         ...   \n",
       "313         313            Zepernick                          S2   \n",
       "314         314              Zeuthen                         S46   \n",
       "315         315            Zitadelle                          U7   \n",
       "316         316  Zoologischer Garten      U2, U9, S3, S5, S7, S9   \n",
       "317         317       Zwickauer Damm                          U7   \n",
       "\n",
       "               area   latitude  longitude              keys  \n",
       "0    Charlottenburg  52.499722  13.307222     adenauerplatz  \n",
       "1         Adlershof  52.434722  13.541389         adlershof  \n",
       "2           Wedding  52.560556  13.334167  afrikanische str  \n",
       "3           Marzahn  52.571667  13.565000       ahrensfelde  \n",
       "4             Mitte  52.521389  13.412639              alex  \n",
       "..              ...        ...        ...               ...  \n",
       "313        Panketal  52.659722  13.533889         zepernick  \n",
       "314         Zeuthen  52.348611  13.627500           zeuthen  \n",
       "315      Haselhorst  52.537778  13.217778         zitadelle  \n",
       "316  Charlottenburg  52.507222  13.332500               zoo  \n",
       "317    Gropiusstadt  52.423333  13.483889    zwickauer damm  \n",
       "\n",
       "[318 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_map = map.merge(key[['station name','keys']],how='left', on='station name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_map = final_map[['station name','lines','area','LAT','LON','Location','color','keys']]\n",
    "final_map.to_csv('final_map.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bvg-controller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
